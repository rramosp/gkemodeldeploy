{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "444a33c6-04c8-4217-a8a1-4be338ad1a74",
   "metadata": {},
   "source": [
    "# monitor cluster\n",
    "\n",
    "by executing commands on all its pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5e55445-d349-4fb7-8e2b-59bb47a2de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "from loguru import logger\n",
    "from time import sleep\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f22cd8c-d41a-48ce-8f20-be916ce97ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pods:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.get_pods()\n",
    "\n",
    "    def get_pods(self):\n",
    "        logger.info('getting pods')\n",
    "        getpods_command = 'kubectl get pods'\n",
    "        \n",
    "        s = subprocess.run(getpods_command.split(), capture_output=True).stdout.decode()\n",
    "        self.podnames = [i.split()[0] for i in s.split('\\n') if not i.startswith('NAME') and len(i.split())>0]\n",
    "\n",
    "    def run_cmd(self, cmd):\n",
    "        def _run_cmd(podname, cmd):\n",
    "            formatted_cmd = cmd.format(podname=podname)\n",
    "            return subprocess.run(formatted_cmd.split(), capture_output=True).stdout.decode().strip()\n",
    "        \n",
    "        r = Parallel(n_jobs=-1)(delayed(_run_cmd)(podname, cmd) for podname in self.podnames)    \n",
    "        return {k:v for k,v in zip(self.podnames, r)}\n",
    "    \n",
    "    def gpu_usage(self):\n",
    "        gpusage_command = 'kubectl exec -it {podname} --  nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader'\n",
    "        return self.run_cmd(gpusage_command)\n",
    "\n",
    "    def run_loop(self, method, wait_seconds=1):\n",
    "        while True:\n",
    "            r = method()\n",
    "            clear_output()\n",
    "            current_dateTime = datetime.now()    \n",
    "            print(current_dateTime)\n",
    "            for k,v in r.items():\n",
    "                print (k, v)\n",
    "\n",
    "            sleep(wait_seconds)\n",
    "            \n",
    "            if np.random.randint(10)==0:\n",
    "                self.get_pods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a4fc05d-6dae-4df1-87f2-af773bff4648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-23 07:11:16.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_pods\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mgetting pods\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pods = Pods()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32d4a67-3356-40da-890d-bc66618c287a",
   "metadata": {},
   "source": [
    "## GPU usage in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32668ef-f001-4cfe-973d-e29ed8b9ccad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-23 08:47:55.610392\n",
      "tgi-gemma-deployment-7d9f9dcd9d-fkhnh 0 %\n"
     ]
    }
   ],
   "source": [
    "pods.run_loop(pods.gpu_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e6badc-4722-42b5-9214-2ec8fe6dfe06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15c429b-bb4d-4248-b910-c763b32d68ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a66219a-e31b-4b9b-bf2f-20d1d605d628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f5727f-5522-46dd-8d10-afd3a5386f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546ee4b8-5951-4edb-8d5e-c8282d414bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c14d88ea-2cc7-44f1-bfa3-8701db43e055",
   "metadata": {},
   "outputs": [],
   "source": [
    "k =   [\n",
    "    {\n",
    "      \"kind\": \"Deployment\",\n",
    "      \"apiVersion\": \"apps/v1\",\n",
    "      \"content\": \"apiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  annotations:\\n    aire.gke.io/generated: \\\"true\\\"\\n    aire.gke.io/inference-server: vllm\\n  creationTimestamp: null\\n  labels:\\n    app: llama3-8b-vllm-inference-server\\n  name: llama3-8b-vllm-deployment\\n  namespace: default\\nspec:\\n  replicas: 1\\n  selector:\\n    matchLabels:\\n      app: llama3-8b-vllm-inference-server\\n  strategy: {}\\n  template:\\n    metadata:\\n      creationTimestamp: null\\n      labels:\\n        ai.gke.io/inference-server: vllm\\n        ai.gke.io/model: LLaMA3_8B\\n        app: llama3-8b-vllm-inference-server\\n        examples.ai.gke.io/source: blueprints\\n    spec:\\n      containers:\\n      - args:\\n        - --model=$(MODEL_ID)\\n        command:\\n        - python3\\n        - -m\\n        - vllm.entrypoints.openai.api_server\\n        env:\\n        - name: MODEL_ID\\n          value: meta-llama/Meta-Llama-3-8B\\n        - name: HUGGING_FACE_HUB_TOKEN\\n          valueFrom:\\n            secretKeyRef:\\n              key: hf_api_token\\n              name: hf-secret\\n        image: vllm/vllm-openai:v0.7.2\\n        name: inference-server\\n        ports:\\n        - containerPort: 8000\\n          name: metrics\\n        readinessProbe:\\n          failureThreshold: 60\\n          httpGet:\\n            path: /health\\n            port: 8000\\n          periodSeconds: 10\\n        resources:\\n          limits:\\n            nvidia.com/gpu: \\\"1\\\"\\n          requests:\\n            nvidia.com/gpu: \\\"1\\\"\\n        volumeMounts:\\n        - mountPath: /dev/shm\\n          name: dshm\\n      nodeSelector:\\n        cloud.google.com/gke-accelerator: nvidia-l4\\n      volumes:\\n      - emptyDir:\\n          medium: Memory\\n        name: dshm\\nstatus: {}\\n\"\n",
    "    },\n",
    "    {\n",
    "      \"kind\": \"HorizontalPodAutoscaler\",\n",
    "      \"apiVersion\": \"autoscaling/v2\",\n",
    "      \"content\": \"apiVersion: autoscaling/v2\\nkind: HorizontalPodAutoscaler\\nmetadata:\\n  annotations:\\n    aire.gke.io/generated: \\\"true\\\"\\n  creationTimestamp: null\\n  labels:\\n    app: llama3-8b-vllm-inference-server\\n  name: vllm-hpa\\n  namespace: default\\nspec:\\n  maxReplicas: 10\\n  metrics:\\n  - pods:\\n      metric:\\n        name: prometheus.googleapis.com|vllm:gpu_cache_usage_perc|gauge\\n      target:\\n        averageValue: 602m\\n        type: AverageValue\\n    type: Pods\\n  minReplicas: 1\\n  scaleTargetRef:\\n    apiVersion: apps/v1\\n    kind: Deployment\\n    name: llama3-8b-vllm-deployment\\nstatus:\\n  currentMetrics: null\\n  desiredReplicas: 0\\n\"\n",
    "    },\n",
    "    {\n",
    "      \"kind\": \"Service\",\n",
    "      \"apiVersion\": \"v1\",\n",
    "      \"content\": \"apiVersion: v1\\nkind: Service\\nmetadata:\\n  annotations:\\n    aire.gke.io/generated: \\\"true\\\"\\n  creationTimestamp: null\\n  labels:\\n    app: llama3-8b-vllm-inference-server\\n  name: llama3-8b-vllm-service\\n  namespace: default\\nspec:\\n  ports:\\n  - port: 8000\\n    protocol: TCP\\n    targetPort: 8000\\n  selector:\\n    app: llama3-8b-vllm-inference-server\\n  type: ClusterIP\\nstatus:\\n  loadBalancer: {}\\n\"\n",
    "    },\n",
    "    {\n",
    "      \"kind\": \"PodMonitoring\",\n",
    "      \"apiVersion\": \"monitoring.googleapis.com/v1\",\n",
    "      \"content\": \"apiVersion: monitoring.googleapis.com/v1\\nkind: PodMonitoring\\nmetadata:\\n  annotations:\\n    aire.gke.io/generated: \\\"true\\\"\\n  labels:\\n    app: llama3-8b-vllm-inference-server\\n  name: vllm-podmonitoring\\n  namespace: default\\nspec:\\n  endpoints:\\n  - interval: 15s\\n    path: /metrics\\n    port: metrics\\n  selector:\\n    matchLabels:\\n      app: llama3-8b-vllm-inference-server\\n  targetLabels:\\n    metadata:\\n    - pod\\n    - container\\n    - node\\n\"\n",
    "    }\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a965727b-b582-4cb4-a061-e7229bc889bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  annotations:\n",
      "    aire.gke.io/generated: \"true\"\n",
      "    aire.gke.io/inference-server: vllm\n",
      "  creationTimestamp: null\n",
      "  labels:\n",
      "    app: llama3-8b-vllm-inference-server\n",
      "  name: llama3-8b-vllm-deployment\n",
      "  namespace: default\n",
      "spec:\n",
      "  replicas: 1\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: llama3-8b-vllm-inference-server\n",
      "  strategy: {}\n",
      "  template:\n",
      "    metadata:\n",
      "      creationTimestamp: null\n",
      "      labels:\n",
      "        ai.gke.io/inference-server: vllm\n",
      "        ai.gke.io/model: LLaMA3_8B\n",
      "        app: llama3-8b-vllm-inference-server\n",
      "        examples.ai.gke.io/source: blueprints\n",
      "    spec:\n",
      "      containers:\n",
      "      - args:\n",
      "        - --model=$(MODEL_ID)\n",
      "        command:\n",
      "        - python3\n",
      "        - -m\n",
      "        - vllm.entrypoints.openai.api_server\n",
      "        env:\n",
      "        - name: MODEL_ID\n",
      "          value: meta-llama/Meta-Llama-3-8B\n",
      "        - name: HUGGING_FACE_HUB_TOKEN\n",
      "          valueFrom:\n",
      "            secretKeyRef:\n",
      "              key: hf_api_token\n",
      "              name: hf-secret\n",
      "        image: vllm/vllm-openai:v0.7.2\n",
      "        name: inference-server\n",
      "        ports:\n",
      "        - containerPort: 8000\n",
      "          name: metrics\n",
      "        readinessProbe:\n",
      "          failureThreshold: 60\n",
      "          httpGet:\n",
      "            path: /health\n",
      "            port: 8000\n",
      "          periodSeconds: 10\n",
      "        resources:\n",
      "          limits:\n",
      "            nvidia.com/gpu: \"1\"\n",
      "          requests:\n",
      "            nvidia.com/gpu: \"1\"\n",
      "        volumeMounts:\n",
      "        - mountPath: /dev/shm\n",
      "          name: dshm\n",
      "      nodeSelector:\n",
      "        cloud.google.com/gke-accelerator: nvidia-l4\n",
      "      volumes:\n",
      "      - emptyDir:\n",
      "          medium: Memory\n",
      "        name: dshm\n",
      "status: {}\n",
      "\n",
      "--\n",
      "apiVersion: autoscaling/v2\n",
      "kind: HorizontalPodAutoscaler\n",
      "metadata:\n",
      "  annotations:\n",
      "    aire.gke.io/generated: \"true\"\n",
      "  creationTimestamp: null\n",
      "  labels:\n",
      "    app: llama3-8b-vllm-inference-server\n",
      "  name: vllm-hpa\n",
      "  namespace: default\n",
      "spec:\n",
      "  maxReplicas: 10\n",
      "  metrics:\n",
      "  - pods:\n",
      "      metric:\n",
      "        name: prometheus.googleapis.com|vllm:gpu_cache_usage_perc|gauge\n",
      "      target:\n",
      "        averageValue: 602m\n",
      "        type: AverageValue\n",
      "    type: Pods\n",
      "  minReplicas: 1\n",
      "  scaleTargetRef:\n",
      "    apiVersion: apps/v1\n",
      "    kind: Deployment\n",
      "    name: llama3-8b-vllm-deployment\n",
      "status:\n",
      "  currentMetrics: null\n",
      "  desiredReplicas: 0\n",
      "\n",
      "--\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  annotations:\n",
      "    aire.gke.io/generated: \"true\"\n",
      "  creationTimestamp: null\n",
      "  labels:\n",
      "    app: llama3-8b-vllm-inference-server\n",
      "  name: llama3-8b-vllm-service\n",
      "  namespace: default\n",
      "spec:\n",
      "  ports:\n",
      "  - port: 8000\n",
      "    protocol: TCP\n",
      "    targetPort: 8000\n",
      "  selector:\n",
      "    app: llama3-8b-vllm-inference-server\n",
      "  type: ClusterIP\n",
      "status:\n",
      "  loadBalancer: {}\n",
      "\n",
      "--\n",
      "apiVersion: monitoring.googleapis.com/v1\n",
      "kind: PodMonitoring\n",
      "metadata:\n",
      "  annotations:\n",
      "    aire.gke.io/generated: \"true\"\n",
      "  labels:\n",
      "    app: llama3-8b-vllm-inference-server\n",
      "  name: vllm-podmonitoring\n",
      "  namespace: default\n",
      "spec:\n",
      "  endpoints:\n",
      "  - interval: 15s\n",
      "    path: /metrics\n",
      "    port: metrics\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: llama3-8b-vllm-inference-server\n",
      "  targetLabels:\n",
      "    metadata:\n",
      "    - pod\n",
      "    - container\n",
      "    - node\n",
      "\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "for ki in k:\n",
    "    print (ki['content'])\n",
    "    print('--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf8ee95-bf5e-4f53-8b9c-d264ad18bfb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p312",
   "language": "python",
   "name": "p312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
